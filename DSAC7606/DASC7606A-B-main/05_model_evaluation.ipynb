{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Model Evaluation\n",
    "\n",
    "In this notebook, we will conduct a thorough evaluation of our trained Convolutional Neural Network (CNN) on the unseen test dataset. The goal is to move beyond a single accuracy score and gain a deeper understanding of the model's performance, its strengths, and its weaknesses.\n",
    "\n",
    "We will cover:\n",
    "1.  **Overall Performance:** Calculating test loss and accuracy.\n",
    "2.  **Class-level Metrics:** Generating a detailed classification report with precision, recall, and F1-score.\n",
    "3.  **Advanced Metrics:** Computing Top-K accuracy.\n",
    "4.  **Visualizations:** Plotting a confusion matrix and ROC curves for an intuitive understanding of performance.\n",
    "5.  **Qualitative Analysis:** Visualizing individual predictions and performing an error analysis to identify common misclassifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, we'll import all the necessary libraries and helper functions. This includes `torch` and `torchvision` for data handling and modeling, `sklearn` for evaluation metrics, `matplotlib` for plotting, and our custom scripts for model architecture and evaluation utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch and Torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Data science and plotting libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Custom helper scripts\n",
    "from scripts.model_architectures import SimpleCNN\n",
    "from scripts.evaluation_metrics import (\n",
    "    evaluate_model,\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curves,\n",
    "    visualize_predictions,\n",
    "    top_k_accuracy,\n",
    "    plot_precision_recall_curves, # Although not used in the final version, good to have\n",
    "    plot_calibration_curve,     # Although not used in the final version, good to have\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data and Model Loading\n",
    "\n",
    "Next, we will prepare the test dataset and load our best-performing model checkpoint that was saved during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.1. Load Test Dataset ---\n",
    "\n",
    "# Define the device for computation (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define data transformations for the test set\n",
    "# These should match the validation transformations to ensure consistency\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load the test dataset from the specified directory\n",
    "test_data_dir = \"data/raw/test\"\n",
    "test_dataset = datasets.ImageFolder(root=test_data_dir, transform=test_transforms)\n",
    "\n",
    "# Create a DataLoader for the test set\n",
    "batch_size = 64\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Test dataset loaded from {test_data_dir}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n",
    "class_names = test_dataset.classes\n",
    "print(f\"Classes: {class_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.2. Load Trained Model ---\n",
    "\n",
    "# Initialize the model architecture\n",
    "model = SimpleCNN(num_classes=len(class_names)).to(device)\n",
    "\n",
    "# Load the saved weights from the best model checkpoint\n",
    "checkpoint = torch.load(\"models/best_model.pth\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "# This is crucial as it disables layers like Dropout and BatchNorm's training behavior\n",
    "model.eval()\n",
    "\n",
    "print(\"Best model loaded successfully and set to evaluation mode.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Overall Performance Evaluation\n",
    "\n",
    "We'll start by getting a high-level view of the model's performance on the entire test set using overall loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test loader\n",
    "# This function returns metrics and raw predictions for further analysis\n",
    "test_loss, test_accuracy, all_preds, all_labels, all_probs = evaluate_model(\n",
    "    model, test_loader, nn.CrossEntropyLoss(), device\n",
    ")\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detailed Performance Analysis\n",
    "\n",
    "Now, let's dive deeper into the model's performance with more detailed metrics and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Classification Report\n",
    "\n",
    "The classification report provides key metrics—precision, recall, and F1-score—for each class. This helps us identify if the model is biased towards or struggles with specific categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Top-K Accuracy\n",
    "\n",
    "Top-K accuracy measures if the true label is among the model's top `K` predictions. This is useful in scenarios where the second or third guess might still be contextually relevant.\n",
    "- **Top-1 Accuracy:** The standard accuracy (the top prediction must be correct).\n",
    "- **Top-5 Accuracy:** The true label must be in the top 5 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_acc = top_k_accuracy(all_labels, all_probs, k=1)\n",
    "top5_acc = top_k_accuracy(all_labels, all_probs, k=5)\n",
    "\n",
    "print(f\"Top-1 Accuracy (Exact Match): {top1_acc:.2f}%\")\n",
    "print(f\"Top-5 Accuracy (Correct label in top 5): {top5_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Confusion Matrix\n",
    "\n",
    "The confusion matrix provides a visual representation of the model's predictions versus the actual labels. The diagonal elements show the number of correct predictions for each class, while off-diagonal elements reveal where the model is making mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plot_confusion_matrix(all_labels, all_preds, class_names)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"results/confusion_matrix.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. ROC Curves (Receiver Operating Characteristic)\n",
    "\n",
    "ROC curves illustrate the diagnostic ability of a classifier as its discrimination threshold is varied. For a multi-class problem, we plot one curve per class (one-vs-rest). A curve that bows towards the top-left corner indicates a better-performing classifier. The Area Under the Curve (AUC) summarizes this performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plot_roc_curves(all_labels, all_probs, class_names)\n",
    "plt.title(\"ROC Curves (One-vs-Rest)\")\n",
    "plt.savefig(\"results/roc_curves.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Qualitative Analysis\n",
    "\n",
    "Beyond metrics, it's insightful to look at individual examples to understand the model's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Visualizing Individual Predictions\n",
    "\n",
    "Let's visualize a few sample images from the test set along with their true labels and the model's predictions. This helps us build intuition about the kinds of images the model handles well and where it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model, test_loader, device, class_names, num_samples=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Error Analysis: Most Common Misclassifications\n",
    "\n",
    "By analyzing the confusion matrix numerically, we can programmatically identify the most frequent errors. This can reveal systematic issues, such as confusion between visually similar classes (e.g., 'cat' vs. 'dog', or 'car' vs. 'truck')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw confusion matrix from sklearn\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Set diagonal to zero to focus only on misclassifications\n",
    "np.fill_diagonal(cm, 0)\n",
    "\n",
    "# Find the indices of the largest errors\n",
    "indices = np.dstack(np.unravel_index(np.argsort(cm.ravel()), cm.shape))[0]\n",
    "\n",
    "print(\"Top 10 Most Common Misclassifications:\")\n",
    "print(\"=======================================\")\n",
    "for i, j in reversed(indices[-10:]):\n",
    "    count = cm[i, j]\n",
    "    if count == 0:\n",
    "        continue\n",
    "    print(f\"'{class_names[i]}' misclassified as '{class_names[j]}': {count} times\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving Results and Conclusion\n",
    "\n",
    "Finally, we save all the collected evaluation data—metrics, predictions, and labels—to a file. This allows for easy reloading and comparison with other models in the future without needing to re-run the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate results into a dictionary\n",
    "results = {\n",
    "    \"test_loss\": test_loss,\n",
    "    \"test_accuracy\": test_accuracy,\n",
    "    \"predictions\": all_preds,\n",
    "    \"labels\": all_labels,\n",
    "    \"probabilities\": all_probs,\n",
    "    \"class_names\": class_names,\n",
    "    \"classification_report\": classification_report(\n",
    "        all_labels, all_preds, target_names=class_names, output_dict=True\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Save the results dictionary to a .npy file\n",
    "np.save(\"results/evaluation_results.npy\", results, allow_pickle=True)\n",
    "\n",
    "print(\"Evaluation complete. Results and visualizations have been saved to the 'results/' directory.\")\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.14.5"
   }
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
