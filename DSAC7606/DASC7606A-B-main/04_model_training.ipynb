{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "This notebook orchestrates the training process for our Convolutional Neural Network (CNN). We will load the augmented dataset, configure the model and training parameters, execute the training loop with early stopping, and finally, save the best-performing model for later evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, we import all necessary libraries and set up the environment, including defining the computation device (GPU or CPU) and setting random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import custom modules for model architecture and training utilities\n",
    "from scripts.model_architectures import SimpleCNN\n",
    "from scripts.train_utils import (\n",
    "    train_epoch,\n",
    "    validate_epoch,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set up the computation device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "In this step, we define the data transformations, load the augmented dataset, split it into training and validation sets, and create data loaders to efficiently feed the data to the model. Corresponding function in train_utils.py: `load_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations: resize, convert to tensor, and normalize\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load the full dataset from the augmented data directory\n",
    "train_data_dir = \"data/augmented/train\"\n",
    "full_dataset = datasets.ImageFolder(root=train_data_dir, transform=data_transforms)\n",
    "\n",
    "# Split the dataset into training and validation sets (80/20 split)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Print dataset summary\n",
    "print(f\"Dataset loaded from: {train_data_dir}\")\n",
    "print(f\"Total images: {len(full_dataset)}\")\n",
    "print(f\"Number of classes: {len(full_dataset.classes)}\")\n",
    "print(f\"Class names: {full_dataset.classes}\")\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Configuration\n",
    "\n",
    "Here, we initialize our `SimpleCNN` model and move it to the selected device. We also define the loss function, the optimizer, and a learning rate scheduler to adjust the learning rate based on validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and move it to the device\n",
    "model = SimpleCNN(num_classes=10).to(device)\n",
    "\n",
    "# Define the loss function (Criterion)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer with weight decay for regularization\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "# Define a learning rate scheduler to reduce LR on plateau\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Configuration\n",
    "\n",
    "We set the hyperparameters for the training loop, such as the number of epochs and the patience for early stopping. We also initialize variables to track the best model and lists to store the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "num_epochs = 50\n",
    "early_stopping_patience = 5\n",
    "\n",
    "# Initialize tracking variables\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "\n",
    "# Lists to store training history for later plotting\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Create directories for saving models and results if they don't exist\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "print(f\"Training configured for {num_epochs} epochs with early stopping patience of {early_stopping_patience}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "This is the main training loop. For each epoch, we train the model on the training set and then evaluate its performance on the validation set. We save the model checkpoint whenever the validation loss improves and stop training early if there is no improvement for a set number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Train for one epoch\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "\n",
    "    # Validate the model\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "\n",
    "    # Update learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Store metrics for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # Check for improvement and save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        save_checkpoint(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"best_val_loss\": best_val_loss,\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "            },\n",
    "            \"models/best_model.pth\",\n",
    "        )\n",
    "        print(\"  ↳ Validation loss improved. Saving best model!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(\n",
    "            f\"  ↳ No improvement. Early stopping counter: {patience_counter}/{early_stopping_patience}\"\n",
    "        )\n",
    "\n",
    "    # Check for early stopping\n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch + 1} epochs!\")\n",
    "        break\n",
    "\n",
    "print(\"\\nTraining completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Training Results\n",
    "\n",
    "After training, we save the collected metrics (loss and accuracy) and visualize them to understand the model's learning progress and identify potential issues like overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training history to numpy arrays\n",
    "np.save(\"results/train_losses.npy\", np.array(train_losses))\n",
    "np.save(\"results/val_losses.npy\", np.array(val_losses))\n",
    "np.save(\"results/train_accuracies.npy\", np.array(train_accuracies))\n",
    "np.save(\"results/val_accuracies.npy\", np.array(val_accuracies))\n",
    "\n",
    "print(\"Training history saved to 'results/' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation history\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label=\"Training Accuracy\")\n",
    "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/training_history.png\")\n",
    "print(\"Training history plot saved as 'results/training_history.png'.\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Final Model\n",
    "\n",
    "Finally, we load the best-performing checkpoint (based on the lowest validation loss) and save the model's state dictionary. This clean, final model is ready for inference and evaluation in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model checkpoint saved during training\n",
    "checkpoint = torch.load(\"models/best_model.pth\")\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "# Retrieve details from the checkpoint\n",
    "best_epoch = checkpoint[\"epoch\"]\n",
    "best_val_loss_loaded = checkpoint[\"best_val_loss\"]\n",
    "\n",
    "print(f\"Loaded best model from epoch {best_epoch} with validation loss {best_val_loss_loaded:.4f}\")\n",
    "\n",
    "# Save the final model's state_dict for easy use in evaluation/inference\n",
    "torch.save(model.state_dict(), \"models/final_model.pth\")\n",
    "print(\"Final model state_dict saved to 'models/final_model.pth'.\")\n",
    "\n",
    "print(\"\\nModel training complete! Proceed to the next notebook for model evaluation.\")\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.14.5"
   }
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
